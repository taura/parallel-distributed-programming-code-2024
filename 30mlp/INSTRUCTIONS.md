# Detailed Instructions and Hints for the 30mlp Assignment

## Preface

- Applying the language constructs you have learned to execute a loop in parallel using SIMD instructions, multicores, or GPUs is, of course, a fundamental step. However, it is a very basic and somewhat superficial aspect of the task.
- I want you to gain a deeper understanding through this exercise.

- The goal is to become a researcher, engineer, or programmer capable of **analyzing performance**, **reasoning about it**, and applying the **appropriate** optimization at your disposal.

- What does "reasoning about performance" mean? Roughly, it involves:
  - Measuring how much time a particular piece of code **actually** takes, and
  - Comparing it to your understanding of how much time it **should** take.

- The first part involves actual measurement.
- The second part is based on understanding what your code does and how efficiently the machine should be able to execute it.

- You cannot analyze the entire program at once, especially when it consists of many different parts.

- Instead, you typically focus on the most time-consuming part of your code, which is hopefully small.

## Conceptual Workflow

Here is a conceptual workflow:

1. **Identify** the part of the code that takes a significant portion (ideally, most) of the execution time through **profiling**.
   - Programs typically spend most of their time in the innermost loop, so the identified part is often simple enough.
2. **Analyze** the basic performance profile of that part (most typically, the number of cycles per iteration).
3. **Compare** that number to how many cycles it **should** take based on your understanding of the machine's performance.

- The last step is the main challenge. In reality, knowing the exact number of cycles is often very difficult, if not impossible.
- However, measuring the cycles and determining if they are **reasonably close** to the expected performance is often possible and practical.

- Some useful reference numbers:
  - Simple arithmetic (addition/multiplication, etc.): Both CPUs and GPUs have a latency of 4–7 cycles.
  - Complex arithmetic (division, square root, etc.): Dozens of cycles.
  - Memory access: A few cycles (CPU L1 cache) to several hundred cycles (main memory).

- These numbers can guide your investigation in the right direction.

## Detailed Instructions for the Assignment

Your report should include the following, organized so the reader can easily identify each item:

1. **Breakdown of execution time**: Measure how much time each function takes and modify the program to display these times.
2. **Identify the target function**: Based on the results, determine the most time-consuming function and the loop within it to optimize. Identify the "smallest code" that takes a significant amount of time (typically the innermost loop).
3. **Check the assembly code**: Examine the assembly code generated by the compiler to ensure it is not doing anything obviously inefficient.
4. **Make sense of cycles**: Assess whether the number of cycles makes sense or is at least reasonable based on your understanding of the assembly code.

- Perform these steps **before applying any optimization** and again after each optimization.

- You should already have practiced many of these steps in Jupyter notebook experiments. Below are further hints for each step.

## Hints for Each Step

### Step 1: Breakdown of Execution Time

- This step is generally referred to as **profiling**.
- It can be performed in several ways:
  1. Manually insert code to measure execution time (e.g., using a clock function).
  2. Use profiling tools:
     - For CPUs: `gprof`, `perf`, `pperf`, `callgrind`, `hpctoolkit`.
     - For NVIDIA GPUs: `nsight`, `nsight compute`.

- Profiling tools are useful as they usually require no source code modification. However, they come with a learning curve and the risk of misinterpretation if used without understanding how they work and what they measure.
- These tools typically work by sampling program counters (periodically interrupting the program to check where it is executing). They are subject to sampling errors and errors from code optimizations.

- Manually inserting timing code is more tedious but provides a simple, transparent, and versatile approach when feasible.
- For the simple MLP code in this assignment, manual timing is entirely feasible. Use something like this:

```c
t0 = get_clock();
f();
t1 = get_clock();
g();
t2 = get_clock();
h();
t3 = get_clock();
t_f += (t1 - t0);
t_g += (t2 - t1);
t_h += (t3 - t2);
```

- Implement `get_clock()` for both CPUs and GPUs.
- For details, refer to the `pd06_ilp` Jupyter notebook.

- Alternatively, wrap functions to avoid cluttering the code with probes:

```c
T timed_f() {
    t0 = get_time();
    T result = f();
    t1 = get_time();
    t_f += (t1 - t0);
    return result;
}
```

- On GPUs, consider whether to time on the GPU or CPU:
  1. Timing on the GPU is better for reasoning about GPU code performance itself.
  2. Timing on the CPU is useful for capturing overall overhead, including kernel invocation.
  3. Understanding kernel invocation overhead is important in its own right.

### Step 2: Identify the Target Function

- This step should be straightforward once Step 1 is complete.
- For the MLP code in this assignment, the target will be obvious.


Here is the revised section with your original details preserved and improved for clarity:

---

### Step 3: Check the Assembly Code

- Examine the generated assembly code corresponding to the innermost loop you are interested in.
- Use the trick of inserting inline assembly comments (`asm volatile ("...")`) so the corresponding assembly code can be easily identified.
- You might wonder what's the point of looking at assembly code, especially if you are not very familiar with it.
- You don't have to go through every single instruction to get meaningful information from the assembly.
- A few main points of interest are:
  - Whether a variable is assigned to a register or not (register allocation can significantly impact performance).
  - GPUs lack aggressive instruction-level parallelism, so the instruction count is often a crude but useful indicator of code efficiency.

- Generating assembly code:
  - **For CPUs**: Use the `-S` compiler option, as done in Jupyter notebook exercises.
  - **For GPUs**: Generating PTX varies by compiler:
    - **Clang (OpenMP)**:
      - Add `-save-temps` option in addition to `-S`.
      - You get PTX code in `???-openmp-nvptx64-nvidia-cuda.s` along with many other files.
      - Example:
        ```bash
        clang++ -fopenmp -fopenmp-targets=nvptx64 -S -save-temps a.cc
        ```
    - **NVC (OpenMP)**:
      - Add `-gpu=keep` option in addition to `-S`.
      - You get PTX code in `???.n001.ptx` along with many other files.
      - Note:  For reasons I don't know, the offloaded code may not always appear in the PTX file especially with high optimization levels. Adding the `__device__` keyword to the function of interest and including the `-cuda` option can work as a workaround.
      - Example:
        ```bash
        nvc++ -mp=gpu -cuda -gpu=keep -S a.cc
        ```
    - **NVCC (CUDA)**:
      - Use the `--ptx` option.
      - Example:
        ```bash
        nvcc --ptx a.cu
        ```

### Step 4: Make Sense of Cycles

- Once you identify the assembly sequence for the loop of interest, assess its cycles per iteration.
- Determine whether the observed cycles align with your expectations based on the assembly code.
- Use reference materials, such as the Intel Intrinsics Guide, to understand instruction latencies and throughputs.

---

### Optimizations

- By this point, you should have a solid understanding of **what** the code does, **how many** cycles it actually takes, and hopefully, **why** it takes as many cycles as you observed
- This understanding is more important than simply making the code faster through arbitrary methods.
- Knowing **why** the code performs as it does helps you choose effective optimization directions and expect their outcomes.

- Specific optimizations to try:
  - **On CPUs**:
    - Focus on single-core performance, then parallelize:
      - SIMD
      - Instruction-level parallelism (ILP)
      - Multicore processing
  - **On GPUs**:
    - Start with a single thread, then gradually scale:
      - Single thread
      - Single warp (32 threads)
      - Single thread block (a single SM)
      - Multiple thread blocks (entire device with multiple SMs)

- After each optimization, measure cycles, examine the assembly code, and reason about the results.


Here’s the proofread and revised version of your text:

## How to Submit Your Report

- Write your report by adding the necessary markdown and code cells in `pd30_mlp.sos.ipynb`.
- Make your code available under `~/notebooks/30mlp/parallel-distributed-programming-code-2024`. Cloning the repository into `~/notebooks/30mlp/` and performing your work there will do.
- I recommend and expect that most of your coding work will be done in your preferred text editor (e.g., VS Code, Emacs, Vim, etc.) and/or via a terminal (SSH), rather than directly editing programs in the notebook running in a browser.
- When finished, submit the notebook by pressing the "Submit" button and send a note through UTOL, as you have done for earlier exercises (e.g., `pd01_omp`).
- The "Submit" button in Jupyter should automatically submit your code in `~/notebooks/30mlp/parallel-distributed-programming-code-2024` (or any file under `~/notebooks/30mlp/`).
- **Note**: Unlike previous coding exercises, `pd30_mlp.sos.ipynb` should be a self-contained and readable document that includes all your efforts, thoughts, and experimental results.

### The report should at least include:

1. An investigation of the most time-consuming function (denoted as $F$).
2. The cycles per innermost iteration of $F$ by a single thread **before** applying any optimizations, both on CPU and GPU.
3. The cycles per innermost iteration of $F$ under the following scenarios:
   - Using SIMD (single core of CPU)
   - Using ILP (single core of CPU)
   - Using SIMD + ILP (single core of CPU)
   - Using SIMD + ILP + multicore (multiple CPU cores)
   - Using a single warp (GPU)
   - Using a single thread block (a single streaming multiprocessor of the GPU)
   - Using multiple thread blocks (multiple streaming multiprocessors of the GPU)
4. The execution time of each key function constituting the MLP (e.g., Fully Connected (FC), ReLU, and SoftmaxCrossEntropy), both with and without optimizations.

### Recommendations:

- Tabulate or visualize your results as you would when writing a research paper.
- Additional optimizations not explicitly mentioned above are welcome and encouraged.

